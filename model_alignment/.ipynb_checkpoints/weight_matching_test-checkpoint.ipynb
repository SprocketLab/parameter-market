{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5deb30fa-db28-4373-991d-2e801f8f3307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from re import L\n",
    "import numpy as np\n",
    "from typing import NamedTuple\n",
    "from collections import defaultdict\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a2491394-3cc9-429d-afea-0ec63d1b3359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PermutationSpec(NamedTuple):\n",
    "    perm_to_axes: dict\n",
    "    axes_to_perm: dict\n",
    "    \n",
    "def permutation_spec_from_axes_to_perm(axes_to_perm: dict) -> PermutationSpec:\n",
    "    perm_to_axes = defaultdict(list)\n",
    "    for wk, axis_perms in axes_to_perm.items():\n",
    "        for axis, perm in enumerate(axis_perms):\n",
    "            if perm is not None:\n",
    "                perm_to_axes[perm].append((wk, axis))\n",
    "    return PermutationSpec(perm_to_axes=dict(perm_to_axes), axes_to_perm=axes_to_perm)\n",
    "\n",
    "def mlp_permutation_spec(num_hidden_layers: int) -> PermutationSpec:\n",
    "    \"\"\"We assume that one permutation cannot appear in two axes of the same weight array.\"\"\"\n",
    "    assert num_hidden_layers >= 1\n",
    "    return permutation_spec_from_axes_to_perm({\n",
    "        \"layer0.weight\": (\"P_0\", None),\n",
    "        **{f\"layer{i}.weight\": ( f\"P_{i}\", f\"P_{i-1}\")\n",
    "           for i in range(1, num_hidden_layers)},\n",
    "        **{f\"layer{i}.bias\": (f\"P_{i}\", )\n",
    "           for i in range(num_hidden_layers)},\n",
    "        f\"layer{num_hidden_layers}.weight\": (None, f\"P_{num_hidden_layers-1}\"),\n",
    "        f\"layer{num_hidden_layers}.bias\": (None, ),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "11d2721d-a8f2-47ac-beb3-a06903df89c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_permuted_param(ps: PermutationSpec, perm, k: str, params, except_axis=None):\n",
    "    \"\"\"Get parameter `k` from `params`, with the permutations applied.\"\"\"\n",
    "    w = params[k]\n",
    "    for axis, p in enumerate(ps.axes_to_perm[k]):\n",
    "        # Skip the axis we're trying to permute.\n",
    "        if axis == except_axis:\n",
    "            continue\n",
    "        # None indicates that there is no permutation relevant to that axis.\n",
    "        if p is not None:\n",
    "            w = torch.index_select(w, axis, perm[p].int())\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "80375ebf-6205-4ad4-b71c-f3a079ba4318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_permutation(ps: PermutationSpec, perm, params):\n",
    "    \"\"\"Apply a `perm` to `params`.\"\"\"\n",
    "    return {k: get_permuted_param(ps, perm, k, params) for k in params.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "95ddd1c5-4508-4f2a-9a39-ec796fd847d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_matching(ps: PermutationSpec, params_a, params_b, max_iter=300, init_perm=None):\n",
    "    \n",
    "    \"\"\"Find a permutation of `params_b` to make them match `params_a`.\"\"\"\n",
    "    perm_sizes = {p: params_a[axes[0][0]].shape[axes[0][1]] for p, axes in ps.perm_to_axes.items()}\n",
    "    print(perm_sizes)\n",
    "    perm = {p: torch.arange(n) for p, n in perm_sizes.items()} if init_perm is None else init_perm\n",
    "    \n",
    "    perm_names = list(perm.keys())\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        progress = False\n",
    "        for p_ix in torch.randperm(len(perm_names)):\n",
    "            p = perm_names[p_ix]\n",
    "            n = perm_sizes[p]\n",
    "            A = torch.zeros((n, n))\n",
    "            for wk, axis in ps.perm_to_axes[p]:\n",
    "                w_a = params_a[wk]\n",
    "                w_b = get_permuted_param(ps, perm, wk, params_b, except_axis=axis)\n",
    "                w_a = torch.moveaxis(w_a, axis, 0).reshape((n, -1))\n",
    "                w_b = torch.moveaxis(w_b, axis, 0).reshape((n, -1))\n",
    "                A += w_a @ w_b.T\n",
    "\n",
    "        ri, ci = linear_sum_assignment(A.detach().numpy(), maximize=True)\n",
    "        assert (torch.tensor(ri) == torch.arange(len(ri))).all()\n",
    "        oldL = torch.einsum('ij,ij->i', A, torch.eye(n)[perm[p].long()]).sum()\n",
    "        newL = torch.einsum('ij,ij->i', A,torch.eye(n)[ci, :]).sum()\n",
    "        print(f\"{iteration} / {p}: {newL - oldL}\")\n",
    "        progress = progress or newL > oldL + 1e-12\n",
    "        perm[p] = torch.Tensor(ci)\n",
    "\n",
    "        if not progress:\n",
    "            break\n",
    "\n",
    "    return perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "80981205-aee7-4f2c-9b93-1e47d1ad6530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_weight_matching():\n",
    "    \"\"\"If we just have a single hidden layer then it should converge after just one step.\"\"\"\n",
    "    ps = mlp_permutation_spec(num_hidden_layers=3)\n",
    "    print(\"axes_to_perm\", ps.axes_to_perm)\n",
    "    print(\"perm_to_axes\", ps.perm_to_axes)\n",
    "    \n",
    "    rng = torch.Generator()\n",
    "    rng.manual_seed(13)\n",
    "    num_hidden = 32\n",
    "    \n",
    "    shapes = {\n",
    "        \"layer0.weight\": (20, num_hidden),\n",
    "        \"layer0.bias\": (num_hidden, ),\n",
    "        \"layer1.weight\": (num_hidden, num_hidden),\n",
    "        \"layer1.bias\": (num_hidden, ),\n",
    "        \"layer2.weight\": (num_hidden, num_hidden),\n",
    "        \"layer2.bias\": (num_hidden, ),\n",
    "        \"layer3.weight\": (num_hidden, 10),\n",
    "        \"layer3.bias\": (10, )\n",
    "    }\n",
    "    \n",
    "    params_a = {k: torch.randn(shape, generator=rng) for k, shape in shapes.items()}\n",
    "    params_b = {k: torch.randn(shape, generator=rng) for k, shape in shapes.items()}\n",
    "    \n",
    "    print(params_a.keys(), params_b.keys())\n",
    "    \n",
    "    perm = weight_matching(ps, params_a, params_b)\n",
    "    print(perm)\n",
    "    \n",
    "    return params_a, params_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "30a43b2a-fb7f-415b-86c3-fd1122fb17ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'layer0.weight': ('P_0', None),\n",
       "  'layer1.weight': ('P_1', 'P_0'),\n",
       "  'layer2.weight': ('P_2', 'P_1'),\n",
       "  'layer0.bias': ('P_0',),\n",
       "  'layer1.bias': ('P_1',),\n",
       "  'layer2.bias': ('P_2',),\n",
       "  'layer3.weight': (None, 'P_2'),\n",
       "  'layer3.bias': (None,)},\n",
       " {'P_0': [('layer0.weight', 0), ('layer1.weight', 1), ('layer0.bias', 0)],\n",
       "  'P_1': [('layer1.weight', 0), ('layer2.weight', 1), ('layer1.bias', 0)],\n",
       "  'P_2': [('layer2.weight', 0), ('layer2.bias', 0), ('layer3.weight', 1)]})"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_permutation_spec(num_hidden_layers=3).axes_to_perm, \\\n",
    "mlp_permutation_spec(num_hidden_layers=3).perm_to_axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "76fc5b81-3ffd-422b-b3d0-9e6f2319c171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axes_to_perm {'layer0.weight': ('P_0', None), 'layer1.weight': ('P_1', 'P_0'), 'layer2.weight': ('P_2', 'P_1'), 'layer0.bias': ('P_0',), 'layer1.bias': ('P_1',), 'layer2.bias': ('P_2',), 'layer3.weight': (None, 'P_2'), 'layer3.bias': (None,)}\n",
      "perm_to_axes {'P_0': [('layer0.weight', 0), ('layer1.weight', 1), ('layer0.bias', 0)], 'P_1': [('layer1.weight', 0), ('layer2.weight', 1), ('layer1.bias', 0)], 'P_2': [('layer2.weight', 0), ('layer2.bias', 0), ('layer3.weight', 1)]}\n",
      "dict_keys(['layer0.weight', 'layer0.bias', 'layer1.weight', 'layer1.bias', 'layer2.weight', 'layer2.bias', 'layer3.weight', 'layer3.bias']) dict_keys(['layer0.weight', 'layer0.bias', 'layer1.weight', 'layer1.bias', 'layer2.weight', 'layer2.bias', 'layer3.weight', 'layer3.bias'])\n",
      "{'P_0': 20, 'P_1': 32, 'P_2': 32}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[20, -1]' is invalid for input of size 1024",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [177]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m param_a, param_b \u001b[38;5;241m=\u001b[39m \u001b[43mtest_weight_matching\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [175]\u001b[0m, in \u001b[0;36mtest_weight_matching\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m params_b \u001b[38;5;241m=\u001b[39m {k: torch\u001b[38;5;241m.\u001b[39mrandn(shape, generator\u001b[38;5;241m=\u001b[39mrng) \u001b[38;5;28;01mfor\u001b[39;00m k, shape \u001b[38;5;129;01min\u001b[39;00m shapes\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(params_a\u001b[38;5;241m.\u001b[39mkeys(), params_b\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m---> 27\u001b[0m perm \u001b[38;5;241m=\u001b[39m \u001b[43mweight_matching\u001b[49m\u001b[43m(\u001b[49m\u001b[43mps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(perm)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m params_a, params_b\n",
      "Input \u001b[0;32mIn [174]\u001b[0m, in \u001b[0;36mweight_matching\u001b[0;34m(ps, params_a, params_b, max_iter, init_perm)\u001b[0m\n\u001b[1;32m     17\u001b[0m w_a \u001b[38;5;241m=\u001b[39m params_a[wk]\n\u001b[1;32m     18\u001b[0m w_b \u001b[38;5;241m=\u001b[39m get_permuted_param(ps, perm, wk, params_b, except_axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m---> 19\u001b[0m w_a \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmoveaxis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m w_b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmoveaxis(w_b, axis, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape((n, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     21\u001b[0m A \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m w_a \u001b[38;5;241m@\u001b[39m w_b\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[20, -1]' is invalid for input of size 1024"
     ]
    }
   ],
   "source": [
    "param_a, param_b = test_weight_matching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3ee628dd-a4f5-4eb7-94bb-a3ab81f76c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_a[\"layer0.weight\"].shape, param_a[\"layer0.bias\"].shape, \\\n",
    "# param_a[\"layer1.weight\"].shape, param_a[\"layer1.bias\"].shape, \\\n",
    "# param_a[\"layer2.weight\"].shape, param_a[\"layer2.bias\"].shape, \\\n",
    "# param_a[\"layer3.weight\"].shape, param_a[\"layer3.bias\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa5fd4-1fee-4680-a3b3-334cdc440b73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3bbbb5-559f-4f55-a325-f4ab5085c602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
